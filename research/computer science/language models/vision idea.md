the papers [[zhangMLLMsKnowWhere2025|MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs]] and [[yuIntroducingVisualPerception2025|Introducing Visual Perception Token into Multimodal Large Language Model]] both consider zooming into the relevant area to enhance the VQA capabilities of [[MLLM]], wether [[wangLSNetSeeLarge2025|LSNet: See Large, Focus Small]] tries to improve lightweight `perceptors` with a [[Focus (optics)|focus]] akin idea (need to read further).