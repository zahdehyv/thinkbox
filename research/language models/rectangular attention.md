---
tags:
  - Attention
  - Transformer
  - Computer_Science_-_Artificial_Intelligence
  - Computer_Science_-_Machine_Learning
  - thesis
aliases:
  - RA
---
in [[wangKBLaMKnowledgeBase2024|KBLaM: Knowledge Base augmented Language Model]] it is proposed rectangular attention to make the transformer model attend the triples, but the triples does not attend to themselves and the rest of the context